# nlp-project
This projects aims to identify how prone large language models (LLMs) are to generate toxic content when prompted, along with the lexical and syntactic structures that trigger toxicity in the predefined LLMs.
